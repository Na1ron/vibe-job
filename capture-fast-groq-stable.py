import numpy as np
import soundcard as sc
import torch
import torchaudio
import threading
import warnings

from groq import Groq
from faster_whisper import WhisperModel
from soundcard.mediafoundation import SoundcardRuntimeWarning

# ===================== WARNINGS =====================

warnings.filterwarnings("ignore", category=SoundcardRuntimeWarning)

# ===================== CONFIG =====================

INPUT_RATE = 44100        # ‚Üì –º–µ–Ω—å—à–µ –Ω–∞–≥—Ä—É–∑–∫–∞
TARGET_RATE = 16000
CHANNEL = 0

FRAME_MS = 40             # ‚Üì –º–µ–Ω—å—à–µ –≤—ã–∑–æ–≤–æ–≤ record
FRAME_SIZE = INPUT_RATE * FRAME_MS // 1000

# Silero VAD
VAD_FRAME = 512
VAD_THRESHOLD = 0.6

# End-of-speech
SILENCE_FRAMES_LIMIT = 15   # ~0.5 —Å–µ–∫
MIN_PHRASE_SEC = 0.7

# Whisper
WHISPER_MODEL = "small"     # üöÄ –º–∞–∫—Å–∏–º—É–º —Å–∫–æ—Ä–æ—Å—Ç–∏
LANGUAGE = "ru"

# Groq
GROQ_MODEL = "llama-3.1-8b-instant"

# =================================================


# ===================== GROQ =====================

GROQ_API_KEY = input("Enter Groq API key: ").strip()
groq_client = Groq(api_key=GROQ_API_KEY)

def send_to_groq(text: str) -> str | None:
    try:
        completion = groq_client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."},
                {"role": "user", "content": text},
            ],
            temperature=0.2,
            max_tokens=512,
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        print("‚ùå Groq error:", e)
        return None


def send_to_groq_async(text: str):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ Groq ‚Äî –ù–ï –±–ª–æ–∫–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ"""
    def worker():
        response = send_to_groq(text)
        if response:
            print("ü§ñ Groq:", response)

    threading.Thread(target=worker, daemon=True).start()


# ===================== LOAD MODELS =====================

print("Loading Silero VAD...")
vad_model, _ = torch.hub.load(
    repo_or_dir="snakers4/silero-vad",
    model="silero_vad",
    trust_repo=True
)
vad_model.eval()

print("Loading Whisper...")
whisper = WhisperModel(
    WHISPER_MODEL,
    device="cpu",
    compute_type="int8"
)

# ===================== AUDIO =====================

speaker = sc.default_speaker()
mic = sc.get_microphone(id=str(speaker.name), include_loopback=True)

vad_buffer = torch.zeros(0)
speech_buffer = []
silence_frames = 0
in_speech = False

print("Realtime listening...")

# ===================== MAIN LOOP =====================

with mic.recorder(samplerate=INPUT_RATE) as recorder:
    while True:
        chunk = recorder.record(numframes=FRAME_SIZE)
        mono = chunk[:, CHANNEL]

        audio = torch.from_numpy(mono).float()
        audio = torchaudio.functional.resample(
            audio, INPUT_RATE, TARGET_RATE
        )

        vad_buffer = torch.cat([vad_buffer, audio])

        while len(vad_buffer) >= VAD_FRAME:
            frame = vad_buffer[:VAD_FRAME]
            vad_buffer = vad_buffer[VAD_FRAME:]

            with torch.no_grad():
                speech_prob = vad_model(
                    frame.unsqueeze(0), TARGET_RATE
                ).item()

            # ---- –∂–¥—ë–º –Ω–∞—á–∞–ª–æ —Ä–µ—á–∏ ----
            if not in_speech:
                if speech_prob >= VAD_THRESHOLD:
                    in_speech = True
                    speech_buffer.append(frame)
                    silence_frames = 0
                continue

            # ---- —Ä–µ—á—å –∏–¥—ë—Ç ----
            if speech_prob >= VAD_THRESHOLD:
                speech_buffer.append(frame)
                silence_frames = 0
            else:
                silence_frames += 1

            duration = sum(len(x) for x in speech_buffer) / TARGET_RATE

            # ---- –∫–æ–Ω–µ—Ü —Ñ—Ä–∞–∑—ã ----
            if in_speech and silence_frames >= SILENCE_FRAMES_LIMIT:
                if duration >= MIN_PHRASE_SEC:
                    audio_chunk = torch.cat(speech_buffer)

                    segments, _ = whisper.transcribe(
                        audio_chunk.numpy(),
                        language=LANGUAGE,
                        beam_size=1,
                        temperature=0,
                        suppress_tokens=[],                # FIX faster-whisper
                        condition_on_previous_text=False,
                        without_timestamps=True,    # —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
                    )

                    text = " ".join(s.text for s in segments).strip()
                    if text:
                        print("üìù STT:", text)

                        # üöÄ –ù–ï –±–ª–æ–∫–∏—Ä—É–µ–º –∞—É–¥–∏–æ
                        send_to_groq_async(text)

                # ---- –ø–æ–ª–Ω—ã–π —Å–±—Ä–æ—Å ----
                speech_buffer.clear()
                silence_frames = 0
                in_speech = False
